# NL2API Configuration
# Copy this to .env and fill in your values
#
# SECURITY: Never commit .env to git. Only .env.example should be tracked.

# =============================================================================
# Docker Infrastructure Credentials
# =============================================================================
# These are used by docker-compose.yml. Change from defaults for production!

# PostgreSQL
POSTGRES_USER=evalkit
POSTGRES_PASSWORD=change_me_in_production
POSTGRES_DB=evalkit

# Redis (leave empty to disable auth in development)
REDIS_PASSWORD=
# If using Redis password, also set the full URL for MCP server:
# ENTITY_MCP_REDIS_URL=redis://:your_password@redis:6379/0

# Grafana
GF_SECURITY_ADMIN_USER=admin
GF_SECURITY_ADMIN_PASSWORD=change_me_in_production

# =============================================================================
# Application Configuration
# =============================================================================

# LLM Provider: "claude" or "openai"
NL2API_LLM_PROVIDER=claude

# Claude (Anthropic) - required if using claude provider
NL2API_ANTHROPIC_API_KEY=sk-ant-api03-...

# OpenAI - required if using openai provider
# NL2API_OPENAI_API_KEY=sk-...

# Model override (optional)
# NL2API_LLM_MODEL=claude-sonnet-4-20250514

# Azure OpenAI (if using azure_openai provider)
# NL2API_AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
# NL2API_AZURE_OPENAI_API_VERSION=2024-02-15-preview

# Database (for RAG and batch storage)
# NL2API_POSTGRES_URL=postgresql://evalkit:evalkit@localhost:5432/evalkit

# =============================================================================
# Phase 5: Scale & Production Settings
# =============================================================================

# Redis Cache (optional, for distributed caching)
# NL2API_REDIS_ENABLED=true
# NL2API_REDIS_URL=redis://localhost:6379/0
# NL2API_REDIS_DEFAULT_TTL_SECONDS=3600
# NL2API_REDIS_ENTITY_CACHE_TTL_SECONDS=86400
# NL2API_REDIS_RAG_CACHE_TTL_SECONDS=3600

# Entity Resolution - External Service
# Circuit breaker protects against cascading failures
# NL2API_ENTITY_RESOLUTION_API_ENDPOINT=https://your-entity-service.com
# NL2API_ENTITY_RESOLUTION_API_KEY=your-api-key
# NL2API_ENTITY_RESOLUTION_TIMEOUT_SECONDS=5.0
# NL2API_ENTITY_RESOLUTION_CIRCUIT_FAILURE_THRESHOLD=5
# NL2API_ENTITY_RESOLUTION_CIRCUIT_RECOVERY_SECONDS=30.0
# NL2API_ENTITY_RESOLUTION_RETRY_MAX_ATTEMPTS=3

# Embedding Rate Limits (for OpenAI embeddings)
# NL2API_EMBEDDING_MAX_CONCURRENT=5
# NL2API_EMBEDDING_REQUESTS_PER_MINUTE=3000

# RAG Indexing (for bulk operations)
# NL2API_RAG_INDEXING_BATCH_SIZE=100
# NL2API_RAG_INDEXING_USE_BULK_INSERT=true
# NL2API_RAG_INDEXING_CHECKPOINT_ENABLED=true

# =============================================================================
# Telemetry & Observability (OpenTelemetry)
# =============================================================================
# Telemetry is enabled automatically when OTEL packages are installed.
# If the collector isn't running, it degrades gracefully (no errors).

# OTLP collector endpoint (run `docker compose up -d` for local stack)
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317

# Service identification
# NL2API_TELEMETRY_SERVICE_NAME=nl2api

# Feature toggles
# NL2API_TELEMETRY_TRACING_ENABLED=true
# NL2API_TELEMETRY_METRICS_ENABLED=true
# NL2API_TELEMETRY_EXPORT_INTERVAL_MS=10000

# Metrics emission backends
# NL2API_METRICS_LOG_ENABLED=true
# NL2API_METRICS_FILE_PATH=/var/log/nl2api/metrics.jsonl
# NL2API_METRICS_OTEL_ENABLED=true

# =============================================================================
# Evaluation / LLM Judge Configuration
# =============================================================================
# LLM-as-Judge for RAG evaluation (context relevance, faithfulness, etc.)

# Provider: "anthropic" (default) or "openai"
# EVAL_LLM_PROVIDER=anthropic

# Model selection (default depends on provider):
# Anthropic: claude-3-5-haiku-20241022, claude-3-5-sonnet-20241022
# OpenAI: gpt-4o-mini, gpt-5-mini, gpt-5-nano, gpt-4o, gpt-5
# EVAL_LLM_MODEL=claude-3-5-haiku-20241022

# API keys are read from the standard env vars:
# Anthropic: ANTHROPIC_API_KEY, NL2API_ANTHROPIC_API_KEY, RAG_UI_ANTHROPIC_API_KEY
# OpenAI: OPENAI_API_KEY, NL2API_OPENAI_API_KEY

# =============================================================================
# Observability Stack URLs (for reference, after docker compose up)
# =============================================================================
# Grafana:     http://localhost:3000 (admin/admin)
# Prometheus:  http://localhost:9090
# Jaeger:      http://localhost:16686
