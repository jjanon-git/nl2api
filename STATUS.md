# Project Status

**Last Updated:** 2026-01-20

## Overview

EvalPlatform is a distributed evaluation framework for testing LLM tool-calling, with an embedded NL2API system for translating natural language queries into LSEG financial API calls.

---

## Current Phase: Phase 2 (EstimatesAgent) - IN PROGRESS

### What's Implemented

| Component | Status | Notes |
|-----------|--------|-------|
| **Phase 1: Infrastructure** | âœ… Complete | |
| â””â”€ LLM Abstraction | âœ… | Claude + OpenAI providers with tool-calling |
| â””â”€ RAG Retriever | âœ… | Hybrid vector + keyword search (pgvector) |
| â””â”€ NL2APIOrchestrator | âœ… | Query classification, entity resolution, agent routing |
| â””â”€ Configuration | âœ… | pydantic-settings with env var support |
| â””â”€ Unit Tests | âœ… | 183 tests for NL2API modules |
| **Phase 2: EstimatesAgent** | ğŸŸ¡ In Progress | |
| â””â”€ EstimatesAgent Implementation | âœ… | Rule-based + LLM fallback |
| â””â”€ Field Code Mappings | âœ… | EPS, revenue, EBITDA, recommendations, etc. |
| â””â”€ Period Detection | âœ… | FY1, FQ1, FY2, etc. |
| â””â”€ Entity Resolution | âœ… | Pattern-based extraction + static mappings |
| â””â”€ Evaluation Adapter | âœ… | Integrates with WaterfallEvaluator |
| â””â”€ Integration Tests | âœ… | 18 tests |
| â””â”€ **Real LLM Evaluation** | ğŸŸ¡ Ready | Script created, needs API key to run |
| **Phase 3: Multi-turn + Clarification** | â³ Not Started | |
| **Phase 4: Remaining Agents** | â³ Not Started | DatastreamAgent, FundamentalsAgent, etc. |
| **Phase 5: Scale & Production** | â³ Not Started | |

---

## Test Coverage

```
Total Unit Tests:     254 passing
â”œâ”€â”€ NL2API Tests:     183 passing
â”‚   â”œâ”€â”€ LLM Protocols:     24
â”‚   â”œâ”€â”€ LLM Providers:     17
â”‚   â”œâ”€â”€ RAG Retriever:     14
â”‚   â”œâ”€â”€ Entity Resolver:   17
â”‚   â”œâ”€â”€ Clarification:     27
â”‚   â”œâ”€â”€ Orchestrator:      19
â”‚   â”œâ”€â”€ EstimatesAgent:    51
â”‚   â””â”€â”€ Eval Adapter:      15
â””â”€â”€ Evaluation Tests:  71 passing

Integration Tests:     18 passing
â”œâ”€â”€ Full Eval:         3
â”œâ”€â”€ Sample Queries:    14
â””â”€â”€ Multi-Company:     1
```

---

## EstimatesAgent Accuracy (Mock LLM - Rule-Based Only)

Last evaluation run against 589 estimates-tagged test cases with **mock LLM** (only measures rule-based extraction):

| Metric | Value |
|--------|-------|
| Exact Match | 21.9% |
| Partial Match | 28.0% |
| No Match | 50.1% |
| Average Precision | 49.79% |
| Average Recall | 30.41% |
| Rule-Based Coverage | 71.1% |
| Rule-Based Recall | 42.12% |

**Important:** These metrics only reflect rule-based pattern matching. LLM fallback cases return mock responses and are not properly evaluated.

---

## Running Real LLM Evaluation

To measure actual accuracy with a real LLM:

```bash
# 1. Set API key
export NL2API_ANTHROPIC_API_KEY="sk-ant-..."

# 2. Run evaluation (default: 50 test cases)
python scripts/run_estimates_eval.py --limit 50

# 3. Full evaluation (takes longer, costs more)
python scripts/run_estimates_eval.py --limit 500

# 4. Use OpenAI instead
export NL2API_LLM_PROVIDER="openai"
export NL2API_OPENAI_API_KEY="sk-..."
python scripts/run_estimates_eval.py --limit 50
```

Results are saved to `estimates_eval_<provider>_<count>.json`.

---

## File Structure

```
evalPlatform/
â”œâ”€â”€ CONTRACTS.py                 # Shared data models
â”œâ”€â”€ STATUS.md                    # This file
â”œâ”€â”€ .env.example                 # Environment variable template
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ nl2api/                  # NL2API System
â”‚   â”‚   â”œâ”€â”€ orchestrator.py      # Main entry point
â”‚   â”‚   â”œâ”€â”€ config.py            # Configuration
â”‚   â”‚   â”œâ”€â”€ models.py            # Response models
â”‚   â”‚   â”œâ”€â”€ llm/                 # LLM providers
â”‚   â”‚   â”‚   â”œâ”€â”€ protocols.py     # LLMProvider protocol
â”‚   â”‚   â”‚   â”œâ”€â”€ claude.py        # ClaudeProvider
â”‚   â”‚   â”‚   â”œâ”€â”€ openai.py        # OpenAIProvider
â”‚   â”‚   â”‚   â””â”€â”€ factory.py       # Provider factory
â”‚   â”‚   â”œâ”€â”€ agents/              # Domain agents
â”‚   â”‚   â”‚   â”œâ”€â”€ protocols.py     # DomainAgent protocol
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py          # BaseDomainAgent
â”‚   â”‚   â”‚   â””â”€â”€ estimates.py     # EstimatesAgent âœ…
â”‚   â”‚   â”œâ”€â”€ rag/                 # RAG retrieval
â”‚   â”‚   â”‚   â”œâ”€â”€ protocols.py     # RAGRetriever protocol
â”‚   â”‚   â”‚   â”œâ”€â”€ retriever.py     # HybridRAGRetriever
â”‚   â”‚   â”‚   â””â”€â”€ indexer.py       # Document indexer
â”‚   â”‚   â”œâ”€â”€ resolution/          # Entity resolution
â”‚   â”‚   â”‚   â”œâ”€â”€ protocols.py     # EntityResolver protocol
â”‚   â”‚   â”‚   â””â”€â”€ resolver.py      # ExternalEntityResolver
â”‚   â”‚   â”œâ”€â”€ clarification/       # Ambiguity handling
â”‚   â”‚   â”‚   â””â”€â”€ detector.py      # AmbiguityDetector
â”‚   â”‚   â””â”€â”€ evaluation/          # Eval integration
â”‚   â”‚       â””â”€â”€ adapter.py       # NL2APITargetAdapter
â”‚   â”‚
â”‚   â”œâ”€â”€ common/storage/          # Shared storage layer
â”‚   â””â”€â”€ evaluation/              # Evaluation pipeline
â”‚       â”œâ”€â”€ core/                # Evaluators
â”‚       â””â”€â”€ batch/               # Batch runner
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_estimates_eval.py    # Real LLM evaluation script
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ unit/nl2api/             # 183 unit tests
    â””â”€â”€ integration/             # 18 integration tests
```

---

## Next Steps

### Immediate (to complete Phase 2)
1. **Run real LLM evaluation** - Get actual accuracy metrics with Claude/OpenAI
2. **Analyze failures** - Identify patterns in LLM failures
3. **Improve prompts** - Enhance system prompt based on failure analysis

### Phase 3 (Multi-turn + Clarification)
1. Implement conversation storage (PostgreSQL)
2. Add query expansion for follow-up queries
3. Integrate external entity resolution API
4. Implement clarification question flow

### Phase 4 (Remaining Agents)
1. DatastreamAgent - price, time series, calculated fields
2. FundamentalsAgent - WC codes, TR codes, financials
3. OfficersAgent - executives, compensation, governance
4. ScreeningAgent - SCREEN expressions, rankings

---

## Environment Setup

```bash
# Install dependencies
pip install -e .

# Start PostgreSQL (for RAG storage)
docker compose up -d

# Set API keys
cp .env.example .env
# Edit .env with your API keys

# Run tests
.venv/bin/python -m pytest tests/unit/ -v
.venv/bin/python -m pytest tests/integration/ -v
```

---

## Known Limitations

1. **Entity Resolution**: Uses static companyâ†’RIC mappings (~30 companies). External API integration planned for Phase 3.

2. **Rule-Based Coverage**: Only covers common field patterns (EPS, revenue, ratings). Complex queries fall back to LLM.

3. **No RAG in Evaluation**: Current evaluation doesn't use RAG retrieval. LLM relies on system prompt examples only.

4. **Single Domain**: Only EstimatesAgent implemented. Other LSEG domains (Datastream, Fundamentals, etc.) pending.
