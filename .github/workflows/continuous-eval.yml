name: Continuous Evaluation (CE)

# This workflow runs comprehensive accuracy evaluation on a schedule
# and tracks accuracy metrics over time.

on:
  schedule:
    # Run tier2 daily at 2 AM UTC (off-peak)
    - cron: '0 2 * * *'
    # Run tier3 comprehensive weekly on Saturday at 2 AM UTC
    - cron: '0 2 * * 6'
  workflow_dispatch:
    inputs:
      tier:
        description: 'Evaluation tier'
        required: true
        default: 'tier2'
        type: choice
        options:
          - tier1
          - tier2
          - tier3
      categories:
        description: 'Categories to evaluate (comma-separated, or "all")'
        required: false
        default: 'all'
        type: string
      notify:
        description: 'Send Slack notification on completion'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.11'

jobs:
  # Determine what to run based on trigger
  setup:
    name: Setup Evaluation
    runs-on: ubuntu-latest
    outputs:
      tier: ${{ steps.config.outputs.tier }}
      categories: ${{ steps.config.outputs.categories }}
      timeout: ${{ steps.config.outputs.timeout }}
    steps:
      - name: Configure evaluation
        id: config
        run: |
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            TIER="${{ inputs.tier }}"
            CATEGORIES="${{ inputs.categories }}"
          elif [[ "${{ github.event.schedule }}" == "0 2 * * 6" ]]; then
            TIER="tier3"
            CATEGORIES="all"
          else
            TIER="tier2"
            CATEGORIES="all"
          fi

          # Set timeout based on tier
          case $TIER in
            tier1) TIMEOUT=15 ;;
            tier2) TIMEOUT=45 ;;
            tier3) TIMEOUT=180 ;;
          esac

          echo "tier=$TIER" >> $GITHUB_OUTPUT
          echo "categories=$CATEGORIES" >> $GITHUB_OUTPUT
          echo "timeout=$TIMEOUT" >> $GITHUB_OUTPUT

          echo "Configuration:"
          echo "  Tier: $TIER"
          echo "  Categories: $CATEGORIES"
          echo "  Timeout: ${TIMEOUT}m"

  # Run the evaluation
  evaluate:
    name: Run Evaluation (${{ needs.setup.outputs.tier }})
    runs-on: ubuntu-latest
    needs: setup
    timeout-minutes: ${{ fromJSON(needs.setup.outputs.timeout) }}

    services:
      postgres:
        image: pgvector/pgvector:pg16
        env:
          POSTGRES_USER: nl2api
          POSTGRES_PASSWORD: nl2api
          POSTGRES_DB: nl2api_eval
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v6

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run database migrations
        env:
          POSTGRES_HOST: localhost
          POSTGRES_PORT: 5432
          POSTGRES_USER: nl2api
          POSTGRES_PASSWORD: nl2api
          POSTGRES_DB: nl2api_eval
        run: |
          # Run migrations if script exists
          if [ -f scripts/run_migrations.py ]; then
            python scripts/run_migrations.py
          fi

      - name: Run Accuracy Evaluation
        id: eval
        env:
          NL2API_ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          POSTGRES_HOST: localhost
          POSTGRES_PORT: 5432
          POSTGRES_USER: nl2api
          POSTGRES_PASSWORD: nl2api
          POSTGRES_DB: nl2api_eval
          OTEL_EXPORTER_OTLP_ENDPOINT: ${{ secrets.OTEL_ENDPOINT }}
        run: |
          TIER="${{ needs.setup.outputs.tier }}"
          CATEGORIES="${{ needs.setup.outputs.categories }}"

          echo "Running $TIER evaluation..."

          # Run pytest with JSON output
          pytest tests/accuracy/ \
            -m "$TIER" \
            -v \
            --tb=short \
            --json-report \
            --json-report-file=eval_results.json \
            2>&1 | tee eval_output.log

          # Extract summary stats
          if [ -f eval_results.json ]; then
            PASSED=$(jq '.summary.passed // 0' eval_results.json)
            FAILED=$(jq '.summary.failed // 0' eval_results.json)
            TOTAL=$((PASSED + FAILED))
            if [ $TOTAL -gt 0 ]; then
              ACCURACY=$(echo "scale=2; $PASSED * 100 / $TOTAL" | bc)
            else
              ACCURACY=0
            fi

            echo "passed=$PASSED" >> $GITHUB_OUTPUT
            echo "failed=$FAILED" >> $GITHUB_OUTPUT
            echo "total=$TOTAL" >> $GITHUB_OUTPUT
            echo "accuracy=$ACCURACY" >> $GITHUB_OUTPUT
          fi

      - name: Generate Report
        if: always()
        run: |
          cat > eval_report.md << 'EOF'
          # Continuous Evaluation Report

          **Date:** $(date -u +"%Y-%m-%d %H:%M UTC")
          **Tier:** ${{ needs.setup.outputs.tier }}
          **Trigger:** ${{ github.event_name }}

          ## Results Summary

          | Metric | Value |
          |--------|-------|
          | Total Tests | ${{ steps.eval.outputs.total || 'N/A' }} |
          | Passed | ${{ steps.eval.outputs.passed || 'N/A' }} |
          | Failed | ${{ steps.eval.outputs.failed || 'N/A' }} |
          | **Accuracy** | **${{ steps.eval.outputs.accuracy || 'N/A' }}%** |

          ## Details

          See attached artifacts for full results.

          EOF

          cat eval_report.md >> $GITHUB_STEP_SUMMARY

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: eval-results-${{ needs.setup.outputs.tier }}-${{ github.run_number }}
          path: |
            eval_results.json
            eval_output.log
            eval_report.md
            routing_eval_*.json
          retention-days: 90

      - name: Check Accuracy Threshold
        if: always()
        run: |
          ACCURACY="${{ steps.eval.outputs.accuracy }}"
          TIER="${{ needs.setup.outputs.tier }}"

          # Set thresholds by tier
          case $TIER in
            tier1) THRESHOLD=80 ;;
            tier2) THRESHOLD=85 ;;
            tier3) THRESHOLD=90 ;;
          esac

          if [ -n "$ACCURACY" ]; then
            PASS=$(echo "$ACCURACY >= $THRESHOLD" | bc)
            if [ "$PASS" -eq 0 ]; then
              echo "::warning::Accuracy ($ACCURACY%) below threshold ($THRESHOLD%)"
            else
              echo "Accuracy ($ACCURACY%) meets threshold ($THRESHOLD%)"
            fi
          fi

  # Store metrics for trend tracking
  track-metrics:
    name: Track Metrics
    runs-on: ubuntu-latest
    needs: [setup, evaluate]
    if: always() && needs.evaluate.result == 'success'
    steps:
      - uses: actions/checkout@v6

      - name: Download results
        uses: actions/download-artifact@v7
        with:
          name: eval-results-${{ needs.setup.outputs.tier }}-${{ github.run_number }}

      - name: Append to metrics history
        run: |
          # Create metrics entry
          METRICS_FILE="metrics/accuracy_history.jsonl"
          mkdir -p metrics

          cat >> $METRICS_FILE << EOF
          {"date": "$(date -u +%Y-%m-%d)", "tier": "${{ needs.setup.outputs.tier }}", "accuracy": ${{ needs.evaluate.outputs.accuracy || 0 }}, "passed": ${{ needs.evaluate.outputs.passed || 0 }}, "failed": ${{ needs.evaluate.outputs.failed || 0 }}, "run_id": "${{ github.run_id }}"}
          EOF

          echo "Metrics appended to $METRICS_FILE"

      # Optional: Commit metrics back to repo
      # - name: Commit metrics
      #   run: |
      #     git config user.name "github-actions[bot]"
      #     git config user.email "github-actions[bot]@users.noreply.github.com"
      #     git add metrics/
      #     git commit -m "chore: update accuracy metrics" || exit 0
      #     git push

  # Optional: Notify on failures
  notify:
    name: Notify
    runs-on: ubuntu-latest
    needs: [setup, evaluate]
    if: failure() || (github.event_name == 'workflow_dispatch' && inputs.notify)
    steps:
      - name: Send notification
        run: |
          echo "Evaluation completed with status: ${{ needs.evaluate.result }}"
          # Add Slack/email notification here if needed
          # Example with Slack:
          # curl -X POST -H 'Content-type: application/json' \
          #   --data '{"text":"CE Pipeline: ${{ needs.evaluate.result }}"}' \
          #   ${{ secrets.SLACK_WEBHOOK_URL }}
